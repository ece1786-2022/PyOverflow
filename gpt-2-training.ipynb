{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-04T17:41:46.921024Z","iopub.execute_input":"2022-12-04T17:41:46.921434Z","iopub.status.idle":"2022-12-04T17:41:46.998266Z","shell.execute_reply.started":"2022-12-04T17:41:46.921355Z","shell.execute_reply":"2022-12-04T17:41:46.997178Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/combined/test_combined.csv\n/kaggle/input/combined/combined.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:41:47.658457Z","iopub.execute_input":"2022-12-04T17:41:47.658838Z","iopub.status.idle":"2022-12-04T17:41:48.024927Z","shell.execute_reply.started":"2022-12-04T17:41:47.658805Z","shell.execute_reply":"2022-12-04T17:41:48.023979Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"csv\", data_files=\"../input/combined/combined.csv\", split=\"train\")\ntest = load_dataset(\"csv\", data_files=\"../input/combined/test_combined.csv\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:41:48.424396Z","iopub.execute_input":"2022-12-04T17:41:48.425068Z","iopub.status.idle":"2022-12-04T17:41:49.967196Z","shell.execute_reply.started":"2022-12-04T17:41:48.425032Z","shell.execute_reply":"2022-12-04T17:41:49.966026Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-416316b76e136610/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc370304376451bbe2197e1e3dbe084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4201c9d28a44a0daa566bd5ce421472"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-416316b76e136610/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-424c78d725883fd3/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b128b435f56f4ced922421e53111a679"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6888e90887640af9386ee76702eb043"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-424c78d725883fd3/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset.features","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:41:49.973893Z","iopub.execute_input":"2022-12-04T17:41:49.974506Z","iopub.status.idle":"2022-12-04T17:41:49.984113Z","shell.execute_reply.started":"2022-12-04T17:41:49.974466Z","shell.execute_reply":"2022-12-04T17:41:49.982880Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'content': Value(dtype='string', id=None),\n 'topic': Value(dtype='string', id=None),\n 'label': Value(dtype='int64', id=None)}"},"metadata":{}}]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:41:53.339440Z","iopub.execute_input":"2022-12-04T17:41:53.339821Z","iopub.status.idle":"2022-12-04T17:41:53.345731Z","shell.execute_reply.started":"2022-12-04T17:41:53.339788Z","shell.execute_reply":"2022-12-04T17:41:53.344672Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Id', 'Body', 'Title', 'Tags', 'text', 'label'],\n    num_rows: 391\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n        return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True)\n\ndef test_tokenize(examples):\n        return tokenizer(examples[\"Title\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:41:55.124311Z","iopub.execute_input":"2022-12-04T17:41:55.124693Z","iopub.status.idle":"2022-12-04T17:41:59.291584Z","shell.execute_reply.started":"2022-12-04T17:41:55.124633Z","shell.execute_reply":"2022-12-04T17:41:59.290701Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c557d4193174439a513ff3076fbc076"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc9c5020837464b8ddf862dfb12f4a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f127b35e48d41b6947521cf8ef94e73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2793feaad0bf40d38f9c3e05df7b19f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f2c59b01f8a4b0cbf6c7b5c365d11e7"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.remove_columns([\"topic\",\"content\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:41:59.293772Z","iopub.execute_input":"2022-12-04T17:41:59.294445Z","iopub.status.idle":"2022-12-04T17:42:02.356128Z","shell.execute_reply.started":"2022-12-04T17:41:59.294400Z","shell.execute_reply":"2022-12-04T17:42:02.355019Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 298\n})"},"metadata":{}}]},{"cell_type":"code","source":"test_tokenized = test.map(test_tokenize, batched=True)\ntest_tokenized = test_tokenized.remove_columns([\"Id\",\"Title\", \"Body\",\"text\",\"Tags\"])\ntest_tokenized = test_tokenized.rename_column(\"label\", \"labels\")\ntest_tokenized.set_format(\"torch\")\ntest_tokenized","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:42:02.357918Z","iopub.execute_input":"2022-12-04T17:42:02.358293Z","iopub.status.idle":"2022-12-04T17:42:02.583754Z","shell.execute_reply.started":"2022-12-04T17:42:02.358257Z","shell.execute_reply":"2022-12-04T17:42:02.582667Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4796bbac4cf479fbbab094616abdb7c"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 391\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = tokenized_datasets.shuffle(seed=42).select(range(0,200))\neval_dataset = tokenized_datasets.shuffle(seed=42).select(range(200,298))\ntest_dataset = test_tokenized.shuffle(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:42:02.586314Z","iopub.execute_input":"2022-12-04T17:42:02.587403Z","iopub.status.idle":"2022-12-04T17:42:02.609209Z","shell.execute_reply.started":"2022-12-04T17:42:02.587363Z","shell.execute_reply":"2022-12-04T17:42:02.608179Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:42:02.612553Z","iopub.execute_input":"2022-12-04T17:42:02.612872Z","iopub.status.idle":"2022-12-04T17:42:02.620045Z","shell.execute_reply.started":"2022-12-04T17:42:02.612842Z","shell.execute_reply":"2022-12-04T17:42:02.619232Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"gpt2-medium\", num_labels=4, vocab_size= vocab_size, pad_token_id=tokenizer.eos_token_id)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:42:02.623296Z","iopub.execute_input":"2022-12-04T17:42:02.623606Z","iopub.status.idle":"2022-12-04T17:43:06.072698Z","shell.execute_reply.started":"2022-12-04T17:42:02.623556Z","shell.execute_reply":"2022-12-04T17:43:06.071668Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc93669a8669497cbcad0279827c6fd8"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=2, per_device_train_batch_size=2, per_device_eval_batch_size=2,logging_dir=\"test_trainer\", evaluation_strategy=\"epoch\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:43:06.074754Z","iopub.execute_input":"2022-12-04T17:43:06.075146Z","iopub.status.idle":"2022-12-04T17:43:13.897192Z","shell.execute_reply.started":"2022-12-04T17:43:06.075102Z","shell.execute_reply":"2022-12-04T17:43:13.896202Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_metric\n\nmetric = load_metric(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:43:50.521711Z","iopub.execute_input":"2022-12-04T17:43:50.522679Z","iopub.status.idle":"2022-12-04T17:43:50.836008Z","shell.execute_reply.started":"2022-12-04T17:43:50.522613Z","shell.execute_reply":"2022-12-04T17:43:50.835100Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:43:53.598492Z","iopub.execute_input":"2022-12-04T17:43:53.598877Z","iopub.status.idle":"2022-12-04T17:43:53.604589Z","shell.execute_reply.started":"2022-12-04T17:43:53.598843Z","shell.execute_reply":"2022-12-04T17:43:53.603456Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=\"/test_trainer\", num_train_epochs=3, per_device_train_batch_size=1, per_device_eval_batch_size=1,logging_dir=\"test_trainer\", evaluation_strategy=\"epoch\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:43:54.209543Z","iopub.execute_input":"2022-12-04T17:43:54.210426Z","iopub.status.idle":"2022-12-04T17:43:54.223854Z","shell.execute_reply.started":"2022-12-04T17:43:54.210390Z","shell.execute_reply":"2022-12-04T17:43:54.222706Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:43:56.119348Z","iopub.execute_input":"2022-12-04T17:43:56.119755Z","iopub.status.idle":"2022-12-04T17:43:56.132493Z","shell.execute_reply.started":"2022-12-04T17:43:56.119718Z","shell.execute_reply":"2022-12-04T17:43:56.131506Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:43:57.649549Z","iopub.execute_input":"2022-12-04T17:43:57.650565Z","iopub.status.idle":"2022-12-04T17:43:57.655493Z","shell.execute_reply.started":"2022-12-04T17:43:57.650525Z","shell.execute_reply":"2022-12-04T17:43:57.654337Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:43:58.954365Z","iopub.execute_input":"2022-12-04T17:43:58.954759Z","iopub.status.idle":"2022-12-04T17:43:58.962033Z","shell.execute_reply.started":"2022-12-04T17:43:58.954725Z","shell.execute_reply":"2022-12-04T17:43:58.961036Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:44:00.206577Z","iopub.execute_input":"2022-12-04T17:44:00.207317Z","iopub.status.idle":"2022-12-04T17:44:00.213033Z","shell.execute_reply.started":"2022-12-04T17:44:00.207280Z","shell.execute_reply":"2022-12-04T17:44:00.211960Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Using device: cuda\n\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:44:01.121581Z","iopub.execute_input":"2022-12-04T17:44:01.122352Z","iopub.status.idle":"2022-12-04T17:51:11.723434Z","shell.execute_reply.started":"2022-12-04T17:44:01.122315Z","shell.execute_reply":"2022-12-04T17:51:11.722525Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 200\n  Num Epochs = 3\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 2\n  Gradient Accumulation steps = 1\n  Total optimization steps = 300\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 06:54, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.549496</td>\n      <td>0.836735</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.454798</td>\n      <td>0.897959</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.399188</td>\n      <td>0.928571</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 98\n  Batch size = 2\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 98\n  Batch size = 2\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 98\n  Batch size = 2\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=300, training_loss=0.600687001546224, metrics={'train_runtime': 430.5495, 'train_samples_per_second': 1.394, 'train_steps_per_second': 0.697, 'total_flos': 1114455932928000.0, 'train_loss': 0.600687001546224, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(test_dataset)\nprint(predictions.predictions.shape, predictions.label_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:51:15.829479Z","iopub.execute_input":"2022-12-04T17:51:15.830129Z","iopub.status.idle":"2022-12-04T17:52:41.645231Z","shell.execute_reply.started":"2022-12-04T17:51:15.830091Z","shell.execute_reply":"2022-12-04T17:52:41.644171Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 391\n  Batch size = 2\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='588' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [196/196 09:14]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"(391, 4) (391,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nlabels = predictions.label_ids\npreds = np.argmax(predictions.predictions, axis=-1)\nmetric.compute(predictions=preds, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:54:37.566631Z","iopub.execute_input":"2022-12-04T17:54:37.567256Z","iopub.status.idle":"2022-12-04T17:54:37.589238Z","shell.execute_reply.started":"2022-12-04T17:54:37.567217Z","shell.execute_reply":"2022-12-04T17:54:37.588344Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.5907928388746803}"},"metadata":{}}]},{"cell_type":"code","source":"def test_tokenize(examples):\n        return tokenizer(examples[\"Body\"], padding=\"max_length\", truncation=True)\ntest_tokenized = test.map(test_tokenize, batched=True)\ntest_tokenized = test_tokenized.remove_columns([\"Id\",\"Title\", \"Body\",\"text\",\"Tags\"])\ntest_tokenized = test_tokenized.rename_column(\"label\", \"labels\")\ntest_tokenized.set_format(\"torch\")\ntest_tokenized\n","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:57:10.150682Z","iopub.execute_input":"2022-12-04T17:57:10.151053Z","iopub.status.idle":"2022-12-04T17:57:10.692608Z","shell.execute_reply.started":"2022-12-04T17:57:10.151016Z","shell.execute_reply":"2022-12-04T17:57:10.691622Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1987f544fa064d0b94c10bc25d178443"}},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 391\n})"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset = test_tokenized.shuffle(seed=42)\npredictions = trainer.predict(test_dataset)\nprint(predictions.predictions.shape, predictions.label_ids.shape)\nimport numpy as np\nlabels = predictions.label_ids\npreds = np.argmax(predictions.predictions, axis=-1)\nmetric.compute(predictions=preds, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:57:15.007438Z","iopub.execute_input":"2022-12-04T17:57:15.007818Z","iopub.status.idle":"2022-12-04T17:58:41.056365Z","shell.execute_reply.started":"2022-12-04T17:57:15.007784Z","shell.execute_reply":"2022-12-04T17:58:41.055381Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 391\n  Batch size = 2\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"(391, 4) (391,)\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.5268542199488491}"},"metadata":{}}]},{"cell_type":"code","source":"def test_tokenize(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\ntest_tokenized = test.map(test_tokenize, batched=True)\ntest_tokenized = test_tokenized.remove_columns([\"Id\",\"Title\", \"Body\",\"text\",\"Tags\"])\ntest_tokenized = test_tokenized.rename_column(\"label\", \"labels\")\ntest_tokenized.set_format(\"torch\")\ntest_tokenized","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:59:01.268519Z","iopub.execute_input":"2022-12-04T17:59:01.269536Z","iopub.status.idle":"2022-12-04T17:59:01.575323Z","shell.execute_reply.started":"2022-12-04T17:59:01.269488Z","shell.execute_reply":"2022-12-04T17:59:01.574267Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1924120ee34cad98a001d6e41da7b3"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 391\n})"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset = test_tokenized.shuffle(seed=42)\npredictions = trainer.predict(test_dataset)\nprint(predictions.predictions.shape, predictions.label_ids.shape)\nimport numpy as np\nlabels = predictions.label_ids\npreds = np.argmax(predictions.predictions, axis=-1)\nmetric.compute(predictions=preds, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:59:04.748993Z","iopub.execute_input":"2022-12-04T17:59:04.749677Z","iopub.status.idle":"2022-12-04T18:00:30.677686Z","shell.execute_reply.started":"2022-12-04T17:59:04.749625Z","shell.execute_reply":"2022-12-04T18:00:30.676561Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 391\n  Batch size = 2\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"(391, 4) (391,)\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.5754475703324808}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}